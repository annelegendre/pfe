# A3C for MCIC

This is an implementation of the Asynchronous Actor-Critic Agents(A3C) method for solving the MCIC problem.
This implementation used as a basis the code at:
https://github.com/awjuliani/DeepRL-Agents/blob/master/A3C-Doom.ipynb


* The folder *CartPole-v0* contains a version of the A3C fot the CartPole-v0 environment from OpenAI Gym
* The folder*MCIC_continuous* contains an unfinished version of the A3C whose Actor network (policy) outputs a normal distribution instead of a value, according to the A3C paper's implementation of the A3C with continuous actions.

## How to run it
```
python a3c_MCIC.py
```

## How to plot the graphs generated by the training
first, specify the episode you want to plot by changing one of the first lines of the plot_cost_const_viol.py file. Then, run the script: 
```
python plot_cost_const_viol.py
```

## A3C dependencies:
* numpy
* matplotlib
* tkinter
* tensorflow
* scipy
* gym

